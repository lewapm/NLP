{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentClassificationOnSNLI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-hOtHmWCSDf"
      },
      "source": [
        "Sentiment classification using LSTM model.\n",
        "Using 220d GloVe embeddings (Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.)\n",
        "Training and testing model on SNLi dataset.\n",
        "\n",
        "To run model put embeddings in ./data/sentiment/word_vectors.txt file and put data (from SNLI dataset) as train.txt and test.txt in ./data/sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWHzb6tZzcjj"
      },
      "source": [
        "import numpy as np\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, PorterStemmer,WordNetLemmatizer\n",
        "import inflect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAP5mNl6h1MC"
      },
      "source": [
        "labels = {'neutral': 0, 'contradiction': 1, 'entailment': 2}\n",
        "words_snli = {}\n",
        "removed_sent = []\n",
        "all_words = 0\n",
        "unk_words = 0\n",
        "word_to_vec = np.array([], dtype=np.float32).reshape(0, 200) \n",
        "word_pos = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRClAzjOkaUv"
      },
      "source": [
        "def to_lowercase(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def normalize(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    return words\n",
        "\n",
        "def add_words_snli(words):\n",
        "  for word in words:\n",
        "    if word not in words_snli:\n",
        "      words_snli[word] = len(words_snli)\n",
        "\n",
        "def read_snli(name):\n",
        "  sentences = []\n",
        "  with open('./data/sentiment/'+name+'.txt', 'r') as test_reader:\n",
        "    first = True\n",
        "    for line in test_reader:\n",
        "      if first == True: \n",
        "        first = False\n",
        "        continue\n",
        "      test_line = line.strip().split('\\t')\n",
        "      label = test_line[0]\n",
        "      sent1 = test_line[5]\n",
        "      sent2 = test_line[6]\n",
        "      if(label == '-'):\n",
        "        if len(removed_sent) < 10:\n",
        "          removed_sent.append((sent1, sent2))\n",
        "        continue\n",
        "      sent1 = normalize(word_tokenize(sent1))\n",
        "      add_words_snli(sent1)\n",
        "      sent2 = normalize(word_tokenize(sent2))\n",
        "      add_words_snli(sent2)\n",
        "      sentences.append((label, sent1, sent2))\n",
        "  return sentences\n",
        "\n",
        "def create_embeddings():\n",
        "  global word_to_vec\n",
        "  global word_pos\n",
        "  word_to_vec = np.random.rand(3, 200)\n",
        "  word_pos = {'xxbos': 0, 'xxeos':1, 'xxunk':2}\n",
        "  with open('./data/sentiment/word_vectors.txt', 'r') as reader:\n",
        "    for line in reader:\n",
        "      line_split = line.strip().split(' ')\n",
        "      if line_split[0] not in words_snli:\n",
        "        continue\n",
        "      word_pos[line_split[0]] = len(word_pos)\n",
        "      word_vec = np.array([float(x) for x in line_split[1:]], dtype=np.float32)\n",
        "      word_to_vec = np.vstack((word_to_vec, word_vec))\n",
        "\n",
        "def adjust_sent(sent):\n",
        "  global unk_words\n",
        "  global all_words\n",
        "  new_sent = ['xxbos']\n",
        "  for word in sent:\n",
        "    if(word not in word_pos):\n",
        "      new_sent.append('xxunk')\n",
        "      unk_words += 1\n",
        "    else:\n",
        "      new_sent.append(word)\n",
        "      all_words += 1\n",
        "  new_sent.append('xxeos')\n",
        "  return new_sent\n",
        "\n",
        "def convert_sents(sents):\n",
        "  new_sents = []\n",
        "  for i, ex in enumerate(sents):\n",
        "    new_sent1 = adjust_sent(ex[1])\n",
        "    new_sent2 = adjust_sent(ex[2])\n",
        "    new_sents.append((ex[0], new_sent1, new_sent2, i))\n",
        "  return new_sents\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u6f2_JXhV1o"
      },
      "source": [
        "train_sent_const = read_snli('train')\n",
        "test_sent_const = read_snli('test')\n",
        "\n",
        "create_embeddings()\n",
        "\n",
        "train_sent = convert_sents(train_sent_const)\n",
        "test_sent = convert_sents(test_sent_const)\n",
        "valid_sent = train_sent[-5000:]\n",
        "train_sent = train_sent[:-5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv1zlgGv96jZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn0zeIhs-BKL"
      },
      "source": [
        "labels = {'neutral': 0, 'contradiction': 1, 'entailment': 2}\n",
        "labels_rev = {0:'neutral', 1:'contradiction', 2: 'entailment'}\n",
        "def prepare_sent(sent):\n",
        "  em_sent = np.array([], dtype=np.float32).reshape(0, 200)\n",
        "  for word in sent:\n",
        "    em_sent = np.vstack((em_sent, word_to_vec[word_pos[word]]))\n",
        "  return em_sent\n",
        "\n",
        "def prepare_tags(examples):\n",
        "  return torch.tensor([labels[ex[0]] for ex in examples], dtype=torch.long, device=torch.device(\"cuda\"))\n",
        "\n",
        "def prepare_sents(sents, no):\n",
        "  sent_lenghts = [len(sent[no]) for sent in sents]\n",
        "  max_sent = np.amax([len(sent[no]) for sent in sents])\n",
        "  embedded_sent = np.array([], dtype=np.float32).reshape(0, max_sent, INPUT_DIM)\n",
        "  for sent in sents:\n",
        "    sent_emb = prepare_sent(sent[no])\n",
        "    while(sent_emb.shape[0] < max_sent):\n",
        "      sent_emb = np.vstack((sent_emb, np.zeros((1, 200))))\n",
        "    embedded_sent = np.vstack((embedded_sent, np.expand_dims(sent_emb, 0)))\n",
        "\n",
        "  return torch.tensor(embedded_sent, dtype=torch.float32, device=torch.device(\"cuda\")), sent_lenghts\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuZIBIeo-JL0"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, batch_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        self.concat2hidden = nn.Linear(2*hidden_dim, 200)\n",
        "        self.hidden2tag = nn.Linear(200, tagset_size)\n",
        "        self.hidden1 = self.init_hidden()\n",
        "        self.hidden2 = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, self.batch_size, self.hidden_dim).cuda(),\n",
        "                torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
        "\n",
        "    def forward(self, sent1, sent1_len, sent2, sent2_len):\n",
        "        X = torch.nn.utils.rnn.pack_padded_sequence(sent1, sent1_len, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out1, self.hidden1 = self.lstm(X, self.hidden1)\n",
        "        Y = torch.nn.utils.rnn.pack_padded_sequence(sent2, sent2_len, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out2, self.hidden2 = self.lstm(Y, self.hidden2)\n",
        "        x = torch.cat((self.hidden1[0], self.hidden2[0]), 2)\n",
        "        x = self.concat2hidden(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.hidden2tag(x)\n",
        "        tag_scores = F.log_softmax(x, dim=1)\n",
        "        return tag_scores.view(self.batch_size, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWbmqgUO3TYQ"
      },
      "source": [
        "false_class = [None, None, None]\n",
        "correct_class = [None, None, None]\n",
        "\n",
        "def eval_model(model, values, name, find=False):\n",
        "  global false_class\n",
        "  global correct_class\n",
        "  if find:\n",
        "    false_class = [None, None, None]\n",
        "    correct_class = [None, None, None]  \n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    with tqdm(total=len(values)//BATCH_SIZE) as pbar:\n",
        "      for pos in range(0, len(values), BATCH_SIZE):\n",
        "          pbar.update(1)\n",
        "          if(pos+BATCH_SIZE > len(values)): continue\n",
        "          ex = values[pos:pos+BATCH_SIZE]\n",
        "          model.hidden1 = model.init_hidden()\n",
        "          model.hidden2 = model.init_hidden()\n",
        "          sent1_emb, sent1_len = prepare_sents(ex, 1)\n",
        "          sent2_emb, sent2_len = prepare_sents(ex, 2)\n",
        "          targets = prepare_tags(ex)\n",
        "          tag_scores = model(sent1_emb, sent1_len, sent2_emb, sent2_len)\n",
        "          idx = torch.argmax(tag_scores, axis=1)\n",
        "          correct += torch.sum(idx == targets)\n",
        "          if find == True:\n",
        "            counter = 0\n",
        "            for i in range(len(targets)):\n",
        "              if idx[i] != targets[i] and false_class[targets[i]] == None:\n",
        "                false_class[targets[i]] = (idx[i].data.numpy(), targets[i].data.numpy(), ex[i][3])\n",
        "              elif idx[i] == targets[i] and correct_class[targets[i]] == None:\n",
        "                correct_class[targets[i]] = (targets[i].data.numpy(), ex[i][3])\n",
        "            for i in range(3):\n",
        "              if false_class[i] != None or correct_class[i] != None: break\n",
        "              else: counter += 2\n",
        "            if counter == 6: find = False\n",
        "    print(\"ACCURACY ON {}: {:.2f}\".format(name, correct.data.numpy()/len(values)*100))\n",
        "  return correct.data.numpy()/len(values)*100\n",
        "\n",
        "def save_model(model, epoch, best_acc, SAVE_PATH, name):\n",
        "  print(\"SAVED model after {} epoch and with valid acc: {}\".format(epoch, best_acc))\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_acc': best_acc\n",
        "            }, SAVE_PATH+name+'.tar')\n",
        "\n",
        "def load_model(SAVE_PATH, name):\n",
        "  model = LSTMTagger(INPUT_DIM, HIDDEN_DIM, BATCH_SIZE, TAGSET_SIZE)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  checkpoint = torch.load(SAVE_PATH+name+'.tar')\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  best_acc = checkpoint['best_acc']\n",
        "\n",
        "  return model, optimizer, epoch, best_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4FOfGhhD7fF"
      },
      "source": [
        "INPUT_DIM = 200\n",
        "HIDDEN_DIM = 200\n",
        "BATCH_SIZE = 64\n",
        "TAGSET_SIZE = 3\n",
        "EPOCHS = 5\n",
        "SAVE_PATH = \"./sentiment/model/\"\n",
        "\n",
        "model = LSTMTagger(INPUT_DIM, HIDDEN_DIM, BATCH_SIZE, TAGSET_SIZE).to(device)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "best_model_acc = 0\n",
        "epoch = 0\n",
        "\n",
        "LOAD = False\n",
        "if LOAD == True:\n",
        "  model, optimizer, epoch, best_model_acc = load_model(SAVE_PATH, 'lstm_model3')\n",
        "\n",
        "valid_acc_tab = []\n",
        "test_acc_tab = []\n",
        "loss_tab = []\n",
        "total_loss_tab = []\n",
        "\n",
        "while epoch < EPOCHS: \n",
        "    epoch += 1\n",
        "    print(\"EPOCH: {}\".format(epoch))\n",
        "    train_sent =  np.random.permutation(train_sent)\n",
        "    total_loss = 0\n",
        "    steps = len(train_sent)//(BATCH_SIZE)\n",
        "    with tqdm(total=steps) as pbar:\n",
        "     for pos in range(0, len(train_sent), BATCH_SIZE):\n",
        "        pbar.update(1)\n",
        "        if(pos+BATCH_SIZE > len(train_sent)): continue\n",
        "        ex = train_sent[pos:pos+BATCH_SIZE]\n",
        "        model.zero_grad()\n",
        "        model.hidden1 = model.init_hidden()\n",
        "        model.hidden2 = model.init_hidden()\n",
        "\n",
        "        sent1_emb, sent1_len = prepare_sents(ex, 1)\n",
        "        sent2_emb, sent2_len = prepare_sents(ex, 2)\n",
        "        targets = prepare_tags(ex)\n",
        "        tag_scores = model(sent1_emb, sent1_len, sent2_emb, sent2_len)\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        total_loss += loss\n",
        "        loss_tab.append(loss.cpu().data.numpy())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"LOSS ON Trainig:\")\n",
        "    plt.plot(loss_tab)\n",
        "    plt.show() \n",
        "    total_loss_tab.append(total_loss.data.numpy()/steps)\n",
        "    print(\"AVARAGE LOSS ON Training:\")\n",
        "    plt.plot(total_loss_tab)\n",
        "    plt.show()\n",
        "    print(\"ACCURACY ON Valid:\")\n",
        "    model_acc = eval_model(model, valid_sent, \"VALID\", True)\n",
        "    valid_acc_tab.append(model_acc)\n",
        "    plt.plot(valid_acc_tab)\n",
        "    plt.show()\n",
        "    print(\"ACCURACY ON Test:\")\n",
        "    test_acc = eval_model(model, test_sent, \"TEST\")\n",
        "    test_acc_tab.append(test_acc)\n",
        "    plt.plot(test_acc_tab)\n",
        "    plt.show()\n",
        "    print(\"ACCURACY VALID: {:.2f}, BEST ACCURACY VALID: {:.2f}, ACCURACY TEST: {:.2f}\".format(model_acc, best_model_acc, test_acc))\n",
        "    if best_model_acc < model_acc:\n",
        "      best_model_acc = model_acc\n",
        "      save_model(model, epoch, best_model_acc, SAVE_PATH, 'lstm_model3')\n",
        "\n",
        "if false_class[0] is not None:\n",
        "  print(\"False classified examples:\")\n",
        "  for x in false_class:\n",
        "    print(\"\"\"Given: {} \n",
        "    and {}. \n",
        "    Classified as {}, should be {}.\\n\"\"\".format(train_sent_const[x[2]][1], train_sent_const[x[2]][2], labels_rev[int(x[0])], labels_rev[int(x[1])]))\n",
        "\n",
        "  print(\"Correctly classified examples\")\n",
        "  for x in correct_class:\n",
        "    print(\"\"\"Given: {} \n",
        "    and {}. \n",
        "    Classified as {}, should be {}.\\n\"\"\".format(train_sent_const[x[1]][1], train_sent_const[x[1]][2], labels_rev[int(x[0])], labels_rev[int(x[0])]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X45DlM1z5o4m"
      },
      "source": [
        "test_acc = eval_model(model, test_sent, \"TEST\")\n",
        "print(test_acc)\n",
        "\n",
        "valid_acc = eval_model(model, valid_sent, \"VALID\")\n",
        "print(valid_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}